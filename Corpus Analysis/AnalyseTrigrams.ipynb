{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import walk\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pickle\n",
    "from nltk.tokenize import WordPunctTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='AES/AES_corpus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read documents as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_doc=[]\n",
    "for i in os.walk(path):\n",
    "    #print(i)\n",
    "    for j in i[2]:\n",
    "        file = open(path+'/'+j, encoding=\"utf8\")\n",
    "        #print(file)\n",
    "        data=json.loads(file.read())\n",
    "        if 'aes_sch_body_t' in data:\n",
    "            list_of_doc.append(data['aes_sch_body_t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26329"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the List to Text File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * For Large Files Split the File in Two, each 15000\n",
    "    * This is needed else we get out of memory issue\n",
    "    Note - This was run on Medium size Instance on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list_of_doc = list_of_doc[:15000]\n",
    "MyList = new_list_of_doc\n",
    "MyFile=open('Corpus_New.txt','w', encoding='utf-8')\n",
    "MyFile.writelines(MyList)\n",
    "MyFile.close()\n",
    "file1 = open(\"Corpus_New.txt\",\"r+\")\n",
    "textfile  = file1.read() \n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(textfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Trigrams Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_tokens = nltk.trigrams(tokens)\n",
    "trigrams_distribution = nltk.FreqDist(trigrams_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on Trigram Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Clean the text\n",
    "    * Display if occurence is above 200\n",
    "    * See if one of the tri gram has has POS as Noun\n",
    "        We can further improvise if make rule if tri gram starts with Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = trigrams_distribution\n",
    "saved_file_trigram = {}\n",
    "for k,v in fdist.items():\n",
    "    trigram = str(' '.join(k))\n",
    "    string_check= re.compile('[©’“›.,@_!#$%^&*()<>?/\\|}{~:]') \n",
    "    if(string_check.search(trigram) == None):\n",
    "        #print(\"String does not contain Special Characters.\")\n",
    "        if v > 200:\n",
    "            token = nlp(trigram)\n",
    "            for t in token:\n",
    "                if t.pos_ == 'NOUN':\n",
    "                    saved_file_trigram.update({trigram:v})\n",
    "                    #print(trigram,v)\n",
    "        else:\n",
    "            pass      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9937"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file1data = saved_file_trigram\n",
    "len(file1data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_file_trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Trigrams for remaining 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sponsor . June 1\n"
     ]
    }
   ],
   "source": [
    "## Part 2\n",
    "\n",
    "##Read the file\n",
    "new_list_of_doc = list_of_doc[15000:]\n",
    "MyList = new_list_of_doc\n",
    "MyFile=open('Corpus_New2.txt','w', encoding='utf-8')\n",
    "MyFile.writelines(MyList)\n",
    "MyFile.close()\n",
    "file1 = open(\"Corpus_New2.txt\",\"r+\")\n",
    "textfile  = file1.read() \n",
    "file1.close()\n",
    "\n",
    "tokens = nltk.word_tokenize(textfile)\n",
    "trigrams_tokens = nltk.trigrams(tokens)\n",
    "trigrams_distribution2 = nltk.FreqDist(trigrams_tokens)\n",
    "\n",
    "fdist = trigrams_distribution2\n",
    "saved_file_trigram = {}\n",
    "print(trigram,v)\n",
    "for k,v in fdist.items():\n",
    "    trigram = str(' '.join(k))\n",
    "    string_check= re.compile('[©’“›.,@_!#$%^&*()<>?/\\|}{~:]') \n",
    "    if(string_check.search(trigram) == None):\n",
    "        #print(\"String does not contain Special Characters.\")\n",
    "        if v > 50:\n",
    "            token = nlp(trigram)\n",
    "            for t in token:\n",
    "                if t.pos_ == 'NOUN':\n",
    "                    saved_file_trigram.update({trigram:v})\n",
    "                    #print(trigram,v)\n",
    "        else:\n",
    "            pass      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9937"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file1data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6810"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file2data = saved_file_trigram\n",
    "len(file2data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15860"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file1data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine both Dictionaries\n",
    "Although This will remove the Duplicates, but in our case we care already filtering based on 200 frequency count and frequency Does'nt make any difference once filter it once.\n",
    "\n",
    "It can be argued, that some feature could have made the case given their counts got split in two halves, but given, we have 11000 features already, it doesnt make significance to lose few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in file2data.items():\n",
    "    file1data.update({k:v})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save file as Xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=file1data, index=[0])\n",
    "df = (df.T)\n",
    "df.to_excel('dict1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the same for Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tokens = nltk.bigrams(tokens)\n",
    "bigram_distribution = nltk.FreqDist(bigram_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create your bigrams\n",
    "bgs = nltk.bigrams(tokens)\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist = nltk.FreqDist(bgs)\n",
    "for k,v in fdist.items():\n",
    "    #print(k,v)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
