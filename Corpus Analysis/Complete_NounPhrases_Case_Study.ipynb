{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Path\n",
    "path=r'/opt/ml-training/Notebooks/Sahil/KnowledgeGraph/DataSet/Case_Study'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Entities\n",
    "def get_entities_update(sent):\n",
    "    \n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "    \n",
    "    \n",
    "    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "    \n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "    \n",
    "    list_ = []\n",
    "    \n",
    "    for tok in nlp(sent):\n",
    "        if tok.dep_ != \"punct\":\n",
    "            if tok.dep_ == \"compound\":\n",
    "                prefix = tok.text\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    prefix = prv_tok_text + \" \"+ tok.text\n",
    "    \n",
    "            if tok.dep_.endswith(\"mod\") == True:\n",
    "                modifier = tok.text\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    modifier = prv_tok_text + \" \"+ tok.text\n",
    "    \n",
    "            if tok.dep_.find(\"subj\") == True:\n",
    "                #ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                ent = ''\n",
    "                ent = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "                list_.append(ent)\n",
    "    \n",
    "            if tok.dep_.find(\"obj\") == True:\n",
    "                ent = ''\n",
    "                ent = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "                list_.append(ent)\n",
    "    \n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "    #return [ent1.strip(), ent2.strip()]\n",
    "    return list_\n",
    "\n",
    "def RemoveStopWords(List_):\n",
    "    #Single_Word_Tokens = []\n",
    "    List_without_space = [x.strip() for x in List_]\n",
    "    for key in List_without_space:\n",
    "        Token = ''.join(key).strip()\n",
    "        Number_of_words = len(Token.split(\" \"))\n",
    "    \n",
    "    #print(key)\n",
    "        if Number_of_words== 1:\n",
    "            #Single_Word_Tokens.append(Token)\n",
    "            doc = nlp(Token) # Convert to Spacy Object\n",
    "            for token in doc:\n",
    "                if token.is_stop:\n",
    "                    token =str(token) # Convert from Spacy to String for list operations\n",
    "                    #print(token)\n",
    "                    try:\n",
    "                        List_without_space.remove(token)\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "                \n",
    "    return List_without_space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_of_Values = []\n",
    "counter = 0\n",
    "for i in os.walk(path):\n",
    "    #print(i)\n",
    "    for j in i[2]:\n",
    "        \n",
    "        StringVal = ''\n",
    "        List_of_Sentences = []\n",
    "        file = open(path+'/'+j, encoding=\"utf8\")\n",
    "        #print(file)\n",
    "        data=json.loads(file.read())\n",
    "        StringVal = data['body_t']\n",
    "        #List_of_Values.append(get_entities_Noun(StringVal))\n",
    "        List_of_Sentences = StringVal.split(\"\\n\")\n",
    "        List_of_Sentences = [re.sub('[^a-zA-Z0-9]+',\" \",x) for x in List_of_Sentences ]\n",
    "        counter  = counter + 1\n",
    "        print(counter)\n",
    "        for each in List_of_Sentences:\n",
    "        #SearchString = str(each) #  = ' 49 EDGE Project Mgmt Work Mgmt INREV Initiate Project Reviews PM10 Project Mgmt Work Mgmt '\n",
    "            if len(each) >1:\n",
    "                List_of_Values.append(get_entities_update(each))\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"List of Values Length:\",len(List_of_Values))\n",
    "#Flatten the List\n",
    "List_consolidated = [val for each in List_of_Values for val in each]\n",
    "print(\"List_consolidated Length:\",len(List_consolidated))\n",
    "# Save Original File\n",
    "with open('CaseStudy.pkl', 'wb') as f:\n",
    "    pickle.dump(List_consolidated, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(List_consolidated)\n",
    "common_words = word_freq.most_common(1000)\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords\n",
    "ConsoladatedList= []\n",
    "ConsoladatedList = RemoveStopWords(List_consolidated)\n",
    "len(ConsoladatedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save StopWords File\n",
    "with open('CaseStudy_SW.pkl', 'wb') as f:\n",
    "    pickle.dump(ConsoladatedList, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('CaseStudy_SW.pkl', 'rb') as f:\n",
    "    ConsoladatedList = pickle.load(f)\n",
    "len(ConsoladatedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Single Characters tokens\n",
    "[ConsoladatedList.remove(each) for each in ConsoladatedList if len(each.strip()) ==1]\n",
    "len(ConsoladatedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConsoladatedList = [re.sub('\\S+@\\S+',\" \",x) for x in ConsoladatedList ] # Replace EmailAddress\n",
    "ConsoladatedList = [re.sub(\"e.g. \",\" \",x) for x in ConsoladatedList]\n",
    "len(ConsoladatedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_Of_Removals = [' ', '', ' ', 'Confidential Proprietary', 'Proprietary', 'Confidential', 'Confidential Proprietary', \n",
    "                    'Don t', 'Status e','g Release', 'N A']\n",
    "[ConsoladatedList.remove(each) for each in ConsoladatedList if each in List_Of_Removals]\n",
    "len(ConsoladatedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in list(ConsoladatedList):\n",
    "    token = x.split(\" \")\n",
    "    if len(token) == 2:\n",
    "        if token[0] == token[1]:\n",
    "            ConsoladatedList.remove(x)\n",
    "\n",
    "len(ConsoladatedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(ConsoladatedList)\n",
    "common_words = word_freq.most_common(1050)\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Two_Tokens_Removals = ['you','now', 'we', 'it', 'that', 'so', 'what', 'this', 'they','us', 'more', 'however', 'when', 'where', \n",
    "                    'then', 'them', 'just', 'll', 'here', 'other', 'which', 'why', 'actually', 'other', 'their', 'how',\n",
    "                     'really', 'there', 'again']\n",
    "\n",
    "for each in list(ConsoladatedList):\n",
    "    token = each.split(\" \")\n",
    "    if token[0].lower() in Two_Tokens_Removals:\n",
    "        ConsoladatedList.remove(each)\n",
    "\n",
    "len(ConsoladatedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(ConsoladatedList)\n",
    "common_words = word_freq.most_common(1050)\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_Of_Removals = ['N ERROR', 'me', 'use', 'way', '0 N ERROR', 'ERROR N ERROR', 'lot', 't i', 'example', 'one  I']\n",
    "[ConsoladatedList.remove(each) for each in ConsoladatedList if each in List_Of_Removals]\n",
    "len(ConsoladatedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(ConsoladatedList)\n",
    "common_words = word_freq.most_common(1050)\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Capture Tags with Length greater than 1\n",
    "Len_Greater1 = []\n",
    "for each in list(ConsoladatedList):\n",
    "    token = each.split(\" \")\n",
    "    if len(token) >1:\n",
    "        Len_Greater1.append(each)\n",
    "len(Len_Greater1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(Len_Greater1)\n",
    "common_words = word_freq.most_common(1050)\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in Len_Greater1:\n",
    "    token = x.split(\" \")\n",
    "    if len(token) == 2:\n",
    "        if token[0] == token[1]:\n",
    "            Len_Greater1.remove(x)\n",
    "\n",
    "len(Len_Greater1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Two_Tokens_Removals = ['you','now', 'we', 'it', 'that', 'so', 'what', 'this', 'they','us', 'more', 'however', 'when', 'where', \n",
    "                    'then', 'them', 'just', 'll', 'here', 'other', 'which', 'why', 'actually', 'other', 'their', 'how',\n",
    "                     'really', 'there', 'again']\n",
    "\n",
    "for each in list(Len_Greater1):\n",
    "    token = each.split(\" \")\n",
    "    if token[0].lower() in Two_Tokens_Removals:\n",
    "        Len_Greater1.remove(each)\n",
    "\n",
    "len(Len_Greater1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "\n",
    "List_Phrase = [phrase for phrase,frequency in common_words]\n",
    "List_Frequency = [frequency for phrase,frequency in common_words]\n",
    "\n",
    "print(len(List_Phrase))\n",
    "print(len(List_Frequency))\n",
    "\n",
    "#for k,v in common_words:\n",
    "#    print(v)\n",
    "#import pandas as pd\n",
    "\n",
    "DF = pd.DataFrame()\n",
    "DF['Phrases'] = List_Phrase\n",
    "DF['Frequency'] = List_Frequency\n",
    "\n",
    "DF.to_excel(\"Client_win_OnlyNoun.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
