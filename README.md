# NLP

Tokenization :

    from nltk.tokenize import sent_tokenize, word_tokenize
    
    from nltk.tokenize import TreebankWordTokenizer 
       
    from nltk.tokenize import WordPunctTokenizer
    
    from nltk.tokenize import RegexpTokenizer 
    
    import spacy
    
    from keras.preprocessing.text import text_to_word_sequence


STEMMING:

    PorterStemmer()
    
    SnowballStemmer()
    
LEMMATIZATION:

    from nltk.stem import WordNetLemmatizer
    
    spacy using token.lemma_
    
    from textblob import TextBlob, Word
    
    
Note : This list is the not the finite list.
    
    
